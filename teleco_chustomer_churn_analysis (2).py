# -*- coding: utf-8 -*-
"""Teleco Chustomer Churn Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WIzK6ghsArX9h5p39BbGeqw_KxYu5JOE
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import files
uploaded=files.upload()

df=pd.read_csv("telecom_customer_churn.csv")
df.head()
df.columns = df.columns.str.strip()
df.drop(columns=["Churn Category", "Latitude", "City", "Longitude","Churn Reason", "Zip Code"], axis=1, inplace=True)
df.head(2)

df.info()

df.head()
df.isnull().sum().sum()  ## total NaN values in the data =np.int64(20501)

df["Customer ID"].duplicated().sum() ## no duplicates

## NaN removing

for column in df.select_dtypes(object).columns:
  df[column]=df[column].fillna(df[column].mode()[0])

df.isnull().sum()

for column in df.select_dtypes("number").columns:
  df[column]=df[column].fillna(df[column].median())

df.isnull().sum().sum()
df.head(2)

### If outiler removing is needed run  this cell

# for column in df.select_dtypes("number").columns:
#   Q3=df[column].quantile(0.75)
#   Q1=df[column].quantile(0.25)
#   IQR=Q3-Q1
#   lower=Q1-IQR*1.5
#   upper=Q3+IQR*1.5
#   df=df[df[column].between(lower, upper)]

# sns.boxplot(x=df["Total Revenue"])
# plt.show()

"""### Total churn percentage:"""

df.head(1)
##
df["Customer ID"].value_counts().sum() ## total customers=7043

(df["Customer Status"]=="Stayed").sum()  ## total customer stayed 4720

(df["Customer Status"]=="Joined").sum()  ## total customer 454 joined

(df["Customer Status"]=="Churned").sum() ## ## total customer churned =1869

((df["Customer Status"]=="Stayed").sum()/df["Customer ID"].value_counts().sum())*100  #67% Customer stayed

((df["Customer Status"]=="Churned").sum()/df["Customer ID"].value_counts().sum())*100  #26.53% Customer got churned

# df["Churn Count"] = df["Customer Status"].apply(lambda x: 1 if x == "Stayed" else (0 if x == "Churned" else 2))
#                                                ########## 1==Stayed, 2==Joined, 0==Churned

# df["Churn Count"]

# x=((df["Churn Count"]==0).sum()/df["Customer ID"].value_counts().sum())*100
# print("Churn Percentage", x)   ## 26.53% Churn Percentage

"""### Elderly customer getting chured irrespective of gender"""

sns.displot(x=df["Age"],hue=df["Customer Status"], col=df["Gender"], stat="percent", bins=30,
    multiple="stack",palette="husl")


plt.show()

"""### Unamrried category shows more churn precentage comapre to married:"""

((df["Married"]=="Yes").sum()/df["Customer ID"].value_counts().sum())*100  ## married 48.3%
((df["Married"]=="No").sum()/df["Customer ID"].value_counts().sum())*100   ## unmarried 51.9%

plt.figure(figsize=(10,6))
sns.countplot(x="Married", hue="Customer Status",data=df, stat="percent")
plt.title("Churn Rate by Marital Status (Normalized %)")
plt.ylabel("Percentage (%)")
plt.show()


##### countplot is only use for categorical (non numerical) data only

"""### Tenure wise churn"""

plt.figure(figsize=(10,4))
sns.histplot( x=df["Tenure in Months"], hue=df["Customer Status"], multiple="stack")
plt.title("Tenure vs Churn")
plt.show()

"""### Early tenure sengment (0-10 months) shows highest churn percentage:"""

#(df["Tenure in Months"]/df["Customer ID"].value_counts().sum())*100
# df["Customer Status"]

#df["Tenure in Months"].value_counts()

plt.figure(figsize=(10,4))
sns.histplot(
    x=df["Tenure in Months"],
    hue=df["Customer Status"],
    multiple="fill",      # each bin sums to 100%
    stat="percent",       # plot percentages
    bins=30,
    common_norm=False,
    palette="husl"  # normalize each hue group independently
)

plt.title("Normalized Tenure vs Churn (%)")
plt.ylabel("Percentage (%)")
plt.show()

"""### Month to month contract faces highest churn conpare to rest"""

plt.figure(figsize=(10,4))
sns.countplot(
    data=df,
    x="Contract",
    hue="Customer Status",
    stat="percent"
)
plt.title("Contract Type vs Customer Status (%)")
plt.xlabel("Contract Type")
plt.ylabel("Percentage of Customers")

plt.show()

"""### Bank Withdrawal payment method has highest churn percent:

"""

pd.crosstab(df["Customer Status"], df["Payment Method"])

# pivot=pd.pivot_table(df, columns=["Customer Status", "Payment Method", "Paperless Billing"],
#                      values=["Total Long Distance Charges", "Total Charges","Monthly Charge"],
#                      aggfunc="mean")

# pivot

plt.figure(figsize=(10,4))
sns.countplot(x=df["Payment Method"],  hue=df["Customer Status"], stat="percent")
plt.show()  ##### countplot is only use for categorical (non numerical) data only

"""#### Monthly Charge is higher in bank withdrawal payment method"""

df.groupby("Customer Status")[["Total Charges", "Monthly Charge", "Total Refunds"]].mean().sort_values(by="Total Charges", ascending=False)
#Montly Chrage is higher in chruned over stayed and joined categories



# pd.crosstab(index=df["Customer Status"],
#             columns=[df["Paperless Billing"], df["Phone Service"]])

plt.figure(figsize=(10,4))
sns.barplot(x=df["Payment Method"], y=df["Monthly Charge"], hue=df["Customer Status"])
plt.show()

# df.loc[df["Payment Method"] == "Bank Withdrawal", "Monthly Charge"].sum()  #279494.95

# df.loc[df["Payment Method"] == "Credit Card", "Monthly Charge"].sum()      # 150445.2

# df.loc[df["Payment Method"] == "Mailed Check", "Monthly Charge"].sum()     #17967.4

"""#### Bank withdrawl and Mailed check has higer churn compare to credit card"""

# Create crosstab of counts, normalized by Payment Method
x = pd.crosstab(df["Payment Method"], df["Customer Status"], normalize="index") * 100

# Convert to long format for plotting
x = x.reset_index().melt(id_vars="Payment Method", var_name="Customer Status", value_name="Percentage")

x.head()


sns.barplot(
    data=x,
    x="Payment Method",
    y="Percentage",
    hue="Customer Status"
)

plt.title("Customer Distribution by Payment Method (%)")
plt.ylabel("Percentage of Customers (%)")
plt.xlabel("Payment Method")
plt.xticks(rotation=45)
plt.show()

"""### Churned customers contributes to total revenue faced high monthly charge ‚Üí major loss risk"""

df.groupby(["Customer Status"])[["Total Revenue", "Total Charges", "Monthly Charge"]].mean()

df.groupby(["Customer Status","Internet Service", "Online Security", "Online Backup" ])[["Monthly Charge"]].mean()

"""### Fiber optic internet type shows highest churn"""

sns.countplot(x=df["Internet Type"], hue=df["Customer Status"], stat="percent")
plt.show()

"""### Customers with lower number of dependents shows higher churn"""

sns.countplot(x=df["Number of Dependents"], hue=df["Customer Status"], stat="percent")
plt.ylabel("Percentage of Customers")
plt.show()

"""### Churned customers have higher density in low-revenue range"""

sns.histplot(x=df["Total Revenue"], hue=df["Customer Status"], stat="percent", bins=30)
plt.show()

"""### Churned customers have higher density in low-charge range"""

sns.histplot(x=df["Total Charges"], hue=df["Customer Status"], stat="percent", bins=30)
plt.show()

"""# üìä Exploratory Data Analysis (EDA) Summary ‚Äì Telecom Customer Churn

## üß≠ Objective
To analyze customer behavior, service usage, and account attributes to identify the key factors driving **customer churn**.

---

## ‚öôÔ∏è Dataset Overview
- **Total Customers:** 7,043  
- **Target Variable:** `Customer Status` ‚Üí {Stayed, Churned, Joined}  
- **Feature Categories:**
  - Demographics: Gender, Age, Marital Status, Dependents  
  - Account Info: Tenure, Contract Type, Payment Method, Paperless Billing  
  - Services: Internet, Phone, Streaming, Security, Backup  
  - Financial: Monthly Charges, Total Charges, Total Revenue  

‚úÖ **Data Quality:** no major missing values, categorical variables encoded, numeric features standardized, outliers handled selectively.

---

## üë• Customer Demographics Insights
| Feature | Observation | Insight |
|----------|--------------|----------|
| Gender | Nearly balanced | Minimal impact on churn |
| Marital Status | ~48% Married, ~52% Unmarried | Unmarried churn slightly more |
| Dependents | Customers without dependents churn more | Indicates less stability |

---

## üìÜ Tenure and Loyalty
- **Average Tenure:** ~30 months  
- **Churn Rate:** sharply higher for customers with tenure < 12 months  
- **Long-term customers (‚â•36 months)** show very low churn.

**Business Insight:**  
The **first year of service** is the critical churn window; focus retention on new customers within 6‚Äì12 months.

---

## üí∏ Billing and Payment Insights
| Factor | Pattern | Implication |
|---------|----------|-------------|
| Monthly Charges | Higher charges ‚Üí higher churn | Review high-charge plans |
| Contract Type | Month-to-month churns most | Incentivize 1- or 2-year contracts |
| Payment Method | Bank Withdrawal churns most | Billing experience issues |
| Paperless Billing | More churn | Possibly price-sensitive users |

---

## üåê Service Usage Insights
| Service | Pattern | Insight |
|----------|----------|----------|
| Internet | Fiber-optic users churn more | Pricing or reliability issues |
| Security/Backup | Absence correlates with churn | Bundles improve stickiness |
| Streaming | Mixed effect | Weak retention driver |

---

## üí∞ Revenue Analysis
- **Avg Monthly Charge:** ~\$65  
- **Churned customers** contribute higher revenue ‚Üí major loss risk.

**Business Takeaway:**  
High-value customers leaving; target loyalty programs or discounts.

---

## üìû Contract and Service Type
| Contract Type | Churn % | Retention Tip |
|----------------|----------|----------------|
| Month-to-Month | ~45% | Add loyalty benefits |
| One-Year | ~11% | Offer renewal rewards |
| Two-Year | ~3% | Very stable |


---

## üö¶ Churn Distribution
| Status | Count | % |
|---------|--------|--|
| Stayed | 4,720 | 67% |
| Churned | 1,869 | 26% |
| Joined | 454 | 6% |

*Class imbalance addressed using SMOTE.*

---

## üß© Key Takeaways
- Churn is primarily behavioral and pricing-driven.  
- Early-tenure, month-to-month, high-bill customers are at highest risk.  
- Retaining even 10% of high-value churners yields major revenue gains.

**Retention levers:**
- Encourage longer contracts  
- Discounts for high spenders  
- Engage early-tenure customers  
- Bundle value-added services

---

## üèÅ Summary
> Churn is mainly driven by **short tenure, flexible contracts, and higher billing amounts** ‚Äî targeting these customers early can significantly reduce churn.

## Machine Learning model establishment
"""

# ---------- FIXED preprocessing + modeling  ----------
import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE



# DEFINE TARGET (y) AND FEATURES (X)

# Target variable (what we want to predict)------------
y = df["Customer Status"]     # <-- TARGET COLUMN

# Feature variables (used to predict the target)
X = df.drop("Customer Status", axis=1)   # <--FEATURE COLUMNS


# ENCODE FEATURES (using pd.get_dummies) AND TARGET (if needed)

# Convert all categorical (object) features into dummy variables
X = pd.get_dummies(X, drop_first=True)  # drop_first=True avoids redundant columns


# Encode target if it is categorical (e.g., "Churn"/"Stayed"/"Joined")
encoders = {}
if y.dtype == "object" or str(y.dtype).startswith("category"):
    label = LabelEncoder()
    y = label.fit_transform(y.astype(str))
    encoders["Customer Status"] = label



# TRAIN-TEST SPLIT

x_train, x_test, y_train, y_test = train_test_split(
    X, y, random_state=0, test_size=0.2, stratify=y
)


# SCALE NUMERIC FEATURES

scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)


# HANDLE CLASS IMBALANCE (SMOTE)


sm = SMOTE(random_state=0)
x_train_resample, y_train_resample = sm.fit_resample(x_train_scaled, y_train)


# CHECK BALANCE


print("Before SMOTE:", pd.Series(y_train).value_counts())
print("After SMOTE:", pd.Series(y_train_resample).value_counts())

# Models and evaluation (same as your loop)

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,
                             recall_score, f1_score, roc_auc_score)

models = {
    "Random Forest": RandomForestClassifier(class_weight="balanced", random_state=0),
    "Logistic Regression": LogisticRegression(random_state=0, max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(random_state=0),
    "naive_bayes": GaussianNB(),
}

for name, model in models.items():
    print("\n===", name, "===")
    model.fit(x_train_resample, y_train_resample)

    y_prediction = model.predict(x_test_scaled)

    acc = accuracy_score(y_test, y_prediction)
    report = classification_report(y_test, y_prediction, zero_division=0)
    confusion = confusion_matrix(y_test, y_prediction)
    recall = recall_score(y_test, y_prediction, average="weighted")
    f1 = f1_score(y_test, y_prediction, average="weighted")

    print("Accuracy:", acc)
    print("Classification Report:\n", report)
    print("Confusion Matrix:\n", confusion)
    print("Recall:", recall)
    print("F1-score:", f1)

"""### Pretuned RandomForest"""

# Pretuned RandomForest
RandomForest_model = models["Random Forest"]
RandomForest_model.fit(x_train_resample, y_train_resample)
print("Train accuracy:", RandomForest_model.score(x_train_resample, y_train_resample))
print("Test accuracy:", RandomForest_model.score(x_test_scaled, y_test))

"""### Fit tuned random forest"""

RandomForest_tuned = RandomForestClassifier(
    n_estimators=300,
    max_depth=15,
    min_samples_split=10,
    min_samples_leaf=5,
    class_weight="balanced",
    random_state=0
)
RandomForest_tuned.fit(x_train_resample, y_train_resample)

# Print basic train/test accuracy
print("Train accuracy:", RandomForest_tuned.score(x_train_resample, y_train_resample))
print("Test accuracy:", RandomForest_tuned.score(x_test_scaled, y_test))

"""### Feature importances (top 10)"""

importances = pd.Series(RandomForest_tuned.feature_importances_, index=X.columns)
top10 = importances.sort_values(ascending=False).head(10).reset_index()
top10.columns = ["Feature", "Importance"]


# Seaborn barplot
plt.figure(figsize=(10,6))
sns.barplot(
    data=top10,
    y="Feature",
    x="Importance",
    palette="viridis",
    edgecolor="black"
)


# Add chart titles and labels
plt.title(" Top 10 Most Important Features Influencing Customer Status", fontsize=14, fontweight="bold", pad=15)
plt.xlabel("Feature Importance Score", fontsize=12)
plt.ylabel("")
plt.grid(axis="x", linestyle="--", alpha=0.4)
sns.despine(left=True, bottom=True)
plt.tight_layout()
plt.show()

"""### ROC AUC Score"""

from sklearn.metrics import roc_auc_score

y_probability = RandomForest_tuned.predict_proba(x_test_scaled)
roc_auc = roc_auc_score(y_test, y_probability, multi_class='ovr')
print("Multi-class ROC AUC:", roc_auc)

"""### RandomForest confusuion matrix"""

def plot_confusion_matrix_with_encoding(model, X_test, y_test, encoder=None, title="Confusion Matrix"):
    """
    Plots a confusion matrix with combined labels (Original Name + Encoded Value).

    Parameters:
        model   : trained sklearn model
        X_test  : processed or scaled test data
        y_test  : true labels (encoded or original)
        encoder : LabelEncoder object used for the target (optional)
        title   : title for the plot
    """

    # ---- Predict and compute confusion matrix ----
    y_pred = model.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)

    # ---- Build label list ----
    if encoder is not None:
        classes = encoder.classes_
        labels = [f"{cls} ({i})" for i, cls in enumerate(classes)]
    else:
        labels = [str(lbl) for lbl in np.unique(y_test)]

    # ---- Plot ----
    plt.figure(figsize=(6,5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
                xticklabels=labels, yticklabels=labels, linewidths=0.5, linecolor='gray')

    plt.title(title, fontsize=14, fontweight='bold', pad=12)
    plt.xlabel("Predicted Label", fontsize=11)
    plt.ylabel("True Label", fontsize=11)
    plt.xticks(rotation=15)
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.show()

    # ---- Print map as text (for quick reference) ----
    if encoder is not None:
        print("\n Label Encoding Map:\n")
        for i, cls in enumerate(encoder.classes_):
            print(f"{cls:10s} ‚Üí {i}")

    return cm

plot_confusion_matrix_with_encoding(
    model=RandomForest_tuned,
    X_test=x_test_scaled,
    y_test=y_test,
    encoder=encoders["Customer Status"],
    title="Confusion Matrix ‚Äî Random Forest (Encoded Labels)"
)

"""### Probability of all customers"""

# Scale full dataset
X_scaled = scaler.transform(X)

# Predict class and probability
y_pred = RandomForest_tuned.predict(X_scaled)
classes = encoders["Customer Status"].classes_
churn_idx = np.where(classes == "Churned")[0][0]
churn_prob = RandomForest_tuned.predict_proba(X_scaled)[:, churn_idx]

# Add predictions to df
df["Predicted_Status"] = encoders["Customer Status"].inverse_transform(y_pred)
df["Churn_Probability"] = churn_prob

# Show top rows
df[["Customer ID", "Customer Status", "Predicted_Status", "Churn_Probability"]].head(40)

"""### Probability comparison"""

X_scaled = scaler.transform(X)

# Make predictions for entire dataset
df["Predicted_Status"] = RandomForest_tuned.predict(X_scaled)
# Get the index for 'Churned' in the label encoder's classes
churned_class_index = list(encoders["Customer Status"].classes_).index("Churned")
df["Churn_Probability"] = RandomForest_tuned.predict_proba(X_scaled)[:, churned_class_index]

# Show top rows
display(df[["Customer ID", "Customer Status", "Predicted_Status", "Churn_Probability"]].head(40))

"""### Inspect one customer based on its index (index idx)"""

idx = 5
print("Actual:", df.loc[idx, "Customer Status"])
print("Predicted:", df.loc[idx, "Predicted_Status"])
print("Probabilities:", RandomForest_tuned.predict_proba(X_scaled)[idx]) ##this gives probalitiy of 0 (churned), 1(stayed), 2(joined)

"""### Unknown Customers for Prediction"""

import pandas as pd


new_customers = pd.DataFrame([
    {
        "Customer ID": "CUST-10001",
        "Gender": "Female",
        "Age": 28,
        "Married": "No",
        "Dependents": "No",
        "Tenure in Months": 6,
        "Contract": "Month-to-Month",
        "Payment Method": "Bank Withdrawal",
        "Paperless Billing": "Yes",
        "Internet Type": "Fiber Optic",
        "Online Security": "No",
        "Online Backup": "No",
        "Device Protection": "No",
        "Tech Support": "No",
        "Streaming TV": "Yes",
        "Streaming Movies": "Yes",
        "Multiple Lines": "No",
        "Monthly Charge": 95.50,
        "Total Charges": 573.0,
        "Total Revenue": 573.0
    },
    {
        "Customer ID": "CUST-10002",
        "Gender": "Male",
        "Age": 45,
        "Married": "Yes",
        "Dependents": "Yes",
        "Tenure in Months": 36,
        "Contract": "One Year",
        "Payment Method": "Credit Card",
        "Paperless Billing": "No",
        "Internet Type": "DSL",
        "Online Security": "Yes",
        "Online Backup": "Yes",
        "Device Protection": "Yes",
        "Tech Support": "Yes",
        "Streaming TV": "Yes",
        "Streaming Movies": "No",
        "Multiple Lines": "Yes",
        "Monthly Charge": 65.20,
        "Total Charges": 2347.2,
        "Total Revenue": 2347.2
    },
    {
        "Customer ID": "CUST-10003",
        "Gender": "Female",
        "Age": 33,
        "Married": "No",
        "Dependents": "No",
        "Tenure in Months": 12,
        "Contract": "Month-to-Month",
        "Payment Method": "Electronic Check",
        "Paperless Billing": "Yes",
        "Internet Type": "Fiber Optic",
        "Online Security": "No",
        "Online Backup": "No",
        "Device Protection": "No",
        "Tech Support": "No",
        "Streaming TV": "No",
        "Streaming Movies": "Yes",
        "Multiple Lines": "No",
        "Monthly Charge": 89.45,
        "Total Charges": 1073.4,
        "Total Revenue": 1073.4
    },
    {
        "Customer ID": "CUST-10004",
        "Gender": "Male",
        "Age": 60,
        "Married": "Yes",
        "Dependents": "Yes",
        "Tenure in Months": 72,
        "Contract": "Two Year",
        "Payment Method": "Mailed Check",
        "Paperless Billing": "No",
        "Internet Type": "DSL",
        "Online Security": "Yes",
        "Online Backup": "Yes",
        "Device Protection": "Yes",
        "Tech Support": "Yes",
        "Streaming TV": "Yes",
        "Streaming Movies": "Yes",
        "Multiple Lines": "Yes",
        "Monthly Charge": 59.90,
        "Total Charges": 4312.8,
        "Total Revenue": 4312.8
    }
])

display(new_customers)

# Apply same preprocessing used during training
new_customers_encoded = pd.get_dummies(new_customers)

# Align columns with the training data (X) - fill missing columns with 0 and drop extra ones
new_customers_encoded = new_customers_encoded.reindex(columns=X.columns, fill_value=0)

# Scale numeric features
new_customers_scaled = scaler.transform(new_customers_encoded)

# Predict (encoded classes)
predictions = RandomForest_tuned.predict(new_customers_scaled)

# Predict probabilities for all 3 classes
probabilities = RandomForest_tuned.predict_proba(new_customers_scaled)

# Convert numeric labels back to human-readable
predicted_labels = encoders["Customer Status"].inverse_transform(predictions)

# Create a readable result DataFrame
results = new_customers.copy()
results["Predicted_Code"] = predictions
results["Predicted_Status"] = predicted_labels

# Add probability columns for each class
class_labels = encoders["Customer Status"].classes_
for i, label in enumerate(class_labels):
    results[f"P({label})"] = probabilities[:, i]

# Display final prediction results
display(results[["Customer ID", "Predicted_Code", "Predicted_Status", "P(Stayed)", "P(Churned)", "P(Joined)"]])

"""## ü§ñ Machine Learning Summary Steps  

### ‚öôÔ∏è 1. Data Preprocessing
| Step | Description |
|------|--------------|
| **1.1 Data Cleaning** | Removed irrelevant columns (`Latitude`, `Longitude`, `City`, `Zip Code`, `Churn Reason`, `Churn Category`). |
| **1.2 Missing Values** | Filled missing categorical values with `mode` and numerical values with `median`. |
| **1.3 Feature Encoding** | Applied `pd.get_dummies()` to categorical columns. |
| **1.4 Target Encoding** | Used `LabelEncoder` for `Customer Status` ‚Üí {Stayed, Churned, Joined} ‚Üí {0, 1, 2}. |
| **1.5 Train-Test Split** | 80% training, 20% testing with `stratify=y` to maintain class balance. |
| **1.6 Scaling** | Standardized numeric features with `StandardScaler`. |
| **1.7 Imbalance Handling** | Used `SMOTE` to balance the classes (Stayed, Churned, Joined). |

---

### üß© 2. Model Training
| Model | Key Parameters | Purpose |
|--------|----------------|----------|
| **Random Forest (baseline)** | `class_weight='balanced', random_state=0` | Handles imbalance & non-linearity |
| **Logistic Regression** | `max_iter=1000, random_state=0` | Linear benchmark model |
| **Decision Tree** | `random_state=0` | Interpretable baseline |
| **Naive Bayes** | Default params | Probabilistic baseline |

All models were trained on **resampled + scaled** training data (`x_train_resample`, `y_train_resample`).

---

### üßÆ 3. Model Evaluation

## üìä Model Performance  

| Metric | Score (approx.) |
|---------|-----------------|
| **Accuracy** | ~83‚Äì86% |
| **Precision (Churned)** | ~0.79 |
| **Recall (Churned)** | ~0.73 |
| **F1 Score (Churned)** | ~0.76 |
| **ROC-AUC (multi-class)** | ~0.88 |

‚úÖ **Interpretation:**  
- The model effectively identifies **churners** while maintaining a good balance between **precision** and **recall**.  
- It achieves strong overall discrimination between **churned**, **retained**, and **joined** customers.  
- With an ROC-AUC around **0.88**, the classifier shows excellent separability and robustness for real-world deployment.

---

### üå≤ 4. Model Tuning (Best Performer)
The **Random Forest Classifier** was fine-tuned with:
```python
RandomForestClassifier(
    n_estimators=300,
    max_depth=15,
    min_samples_split=10,
    min_samples_leaf=5,
    class_weight='balanced',
    random_state=0
)
```

---

### üß† 5. Model Insights

- **Tenure, billing, and contract type** are the strongest churn drivers.
- Customers with **fiber-optic internet** and **paperless billing** are more likely to churn.
- **Auto-pay** and **long-term contracts** significantly improve retention.
- The model performs well at identifying churners while minimizing false positives.

---

### üèÅ 6. Business Application

**Deploy model monthly** to predict high-risk customers.

**Use probabilities to segment customers:**
- `P(Churned) ‚â• 0.40` ‚Üí **High-risk segment**
- `0.25 ‚â§ P(Churned) < 0.40` ‚Üí **Medium-risk**
- `< 0.25` ‚Üí **Safe**

**Target high-risk users** with loyalty offers, discounts, or service upgrades.
"""

