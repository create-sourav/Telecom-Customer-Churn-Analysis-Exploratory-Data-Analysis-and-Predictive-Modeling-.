# ğŸš€ Customer Churn Prediction â€” Production MLOps System

<div align="center">

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.109+-green.svg)
![Hugging Face](https://img.shields.io/badge/ğŸ¤—-Hugging%20Face-yellow.svg)
![CI/CD](https://img.shields.io/badge/CI%2FCD-GitHub%20Actions-blue.svg)
![License](https://img.shields.io/badge/License-MIT-red.svg)

**End-to-end machine learning system that predicts customer churn and recommends retention strategies**

[Live API Demo](https://souravmondal619-churn-mlops-api.hf.space/docs) â€¢ [Report Bug](../../issues) â€¢ [Request Feature](../../issues)

</div>

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Key Features](#-key-features)
- [System Architecture](#ï¸-system-architecture)
- [Project Structure](#-project-structure)
- [Getting Started](#-getting-started)
- [Usage Guide](#-usage-guide)
- [API Documentation](#-api-documentation)
- [CI/CD Pipeline](#-cicd-pipeline)
- [Business Intelligence](#-business-intelligence)
- [Model Details](#-model-details)
- [Deployment](#-deployment)
- [CrewAI Recommendations](#-crewai-recommendation-agent)
- [Security](#-security--best-practices)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)
- [License](#-license)
- [Contact](#-contact)

---

## ğŸ¯ Overview

This project delivers a **production-ready customer churn prediction system** that combines machine learning, automated pipelines, and business intelligence to help organizations:

âœ… **Predict** which customers are likely to churn  
âœ… **Understand** the key drivers behind customer attrition  
âœ… **Act** with AI-powered retention recommendations  

The system transforms raw customer data into actionable insights through an automated ML pipeline that trains models, optimizes decision thresholds, deploys APIs, and generates business reportsâ€”all without manual intervention.

---

## âœ¨ Key Features

### ğŸ¤– Machine Learning
- **Multi-model comparison**: Logistic Regression, Random Forest, Gradient Boosting, XGBoost
- **Optimized decision threshold**: Uses Youden's J statistic instead of default 0.5
- **Feature engineering**: Geospatial clustering, encoding, scaling
- **Model persistence**: All artifacts version-controlled and centralized

### ğŸ”„ MLOps Automation
- **Fully automated CI/CD**: Training â†’ Evaluation â†’ Deployment on every push
- **Zero-downtime deployment**: Models automatically sync to production API
- **Artifact management**: Centralized storage on Hugging Face Model Hub
- **Batch scoring**: Excel input â†’ CSV predictions with business flags

### ğŸŒ Production API
- **FastAPI REST service**: Real-time predictions with automatic docs
- **Hugging Face Spaces hosting**: Scalable, serverless deployment
- **Multi-format output**: Probability scores, binary flags, class labels
- **Swagger UI**: Interactive API testing built-in

### ğŸ“Š Business Intelligence
- **Power BI dashboard**: Visual analytics on churn patterns
- **Explainable insights**: Contract type, tenure, service usage correlations
- **Actionable segments**: High-risk customer identification

### ğŸ¤ AI-Powered Recommendations
- **CrewAI integration**: Generates personalized retention strategies
- **Context-aware suggestions**: Discounts, loyalty rewards, contract upgrades
- **Decision support**: Transforms predictions into business actions

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Raw Dataset   â”‚
â”‚ churn_clean.xlsxâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  preprocess.py  â”‚  â† Data cleaning, encoding, geospatial clustering
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    train.py     â”‚  â† Model training + threshold optimization
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   evaluate.py   â”‚  â† Performance metrics & validation
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                          â”‚
         â–¼                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ batch_predict.py â”‚      â”‚   FastAPI (api.py)  â”‚
â”‚ Excel â†’ CSV      â”‚      â”‚   + HF Spaces       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                          â”‚
         â–¼                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GitHub Artifacts â”‚      â”‚  Live REST Endpoint â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      GitHub Actions CI/CD           â”‚
â”‚  Auto-train â†’ Test â†’ Deploy â†’ Sync â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
MLOps-churn-prediction-system/
â”‚
â”œâ”€â”€ ğŸ“‚ data/
â”‚   â””â”€â”€ churn_clean.xlsx              # Training dataset
â”‚
â”œâ”€â”€ ğŸ“‚ models/                        # Auto-generated artifacts (not committed)
â”‚   â”œâ”€â”€ model.pkl                     # Trained model
â”‚   â”œâ”€â”€ scaler.pkl                    # Feature scaler
â”‚   â”œâ”€â”€ label_encoder.pkl             # Target encoder
â”‚   â”œâ”€â”€ kmeans.pkl                    # Geospatial clustering model
â”‚   â”œâ”€â”€ feature_columns.pkl           # Feature schema
â”‚   â””â”€â”€ churn_threshold.pkl           # Optimized decision boundary
â”‚
â”œâ”€â”€ ğŸ“‚ new_test_data/
â”‚   â”œâ”€â”€ new_data.xlsx                 # Business input for batch predictions
â”‚   â””â”€â”€ predictions_output.csv        # Generated predictions
â”‚
â”œâ”€â”€ ğŸ“‚ src/
â”‚   â”œâ”€â”€ preprocess.py                 # Data preprocessing pipeline
â”‚   â”œâ”€â”€ train.py                      # Model training + optimization
â”‚   â”œâ”€â”€ evaluate.py                   # Model evaluation metrics
â”‚   â”œâ”€â”€ predict.py                    # Single-customer inference
â”‚   â”œâ”€â”€ batch_predict.py              # Bulk prediction pipeline
â”‚   â””â”€â”€ api.py                        # FastAPI service
â”‚
â”œâ”€â”€ ğŸ“‚ notebooks/
â”‚   â””â”€â”€ Teleco_Customer_Churn_Analysis.ipynb  # EDA & experimentation
â”‚
â”œâ”€â”€ ğŸ“‚ dashboards/
â”‚   â””â”€â”€ churn_powerbi.pbix            # Power BI analytics dashboard
â”‚
â”œâ”€â”€ ğŸ“‚ .github/workflows/
â”‚   â””â”€â”€ ci.yml                        # CI/CD automation pipeline
â”‚
â”œâ”€â”€ Dockerfile                        # API containerization
â”œâ”€â”€ requirements.txt                  # Python dependencies
â””â”€â”€ README.md                         # This file
```

---

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8+
- pip package manager
- Git
- (Optional) Docker for containerization

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/MLOps-churn-prediction-system.git
cd MLOps-churn-prediction-system
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Verify data availability**
```bash
# Ensure training data exists
ls data/churn_clean.xlsx
```

### Local Execution

Run the complete pipeline locally:

```bash
# Step 1: Preprocess data
python src/preprocess.py

# Step 2: Train model with threshold optimization
python src/train.py

# Step 3: Evaluate model performance
python src/evaluate.py

# Step 4: Generate batch predictions
python src/batch_predict.py
```

---

## ğŸ“– Usage Guide

### 1ï¸âƒ£ Updating Training Data

**To retrain with new customer data:**

```bash
# 1. Update the training dataset
cp your_new_data.xlsx data/churn_clean.xlsx

# 2. Commit and push changes
git add data/churn_clean.xlsx
git commit -m "Update training data with Q4 2024 customers"
git push origin main
```

**What happens next:**
- GitHub Actions automatically triggers
- Model retrains with new data
- Artifacts upload to Hugging Face
- Production API updates automatically
- No manual deployment needed âœ…

### 2ï¸âƒ£ Batch Predictions (Excel â†’ CSV)

**For business teams to score customer lists:**

```bash
# 1. Place your customer data file
cp customer_list.xlsx new_test_data/new_data.xlsx

# 2. Run batch prediction
python src/batch_predict.py

# 3. Retrieve results
cat new_test_data/predictions_output.csv
```

**Output format:**
```csv
Customer ID,Prediction,Churn_Flag,Churn_Probability,Threshold_Used
0001-BGFD,Joined,YES,0.48,0.25
0002-XKTF,Stayed,NO,0.12,0.25
```

### 3ï¸âƒ£ Single Customer Prediction

```bash
python src/predict.py
```

Provide customer details when prompted, or modify the script for programmatic use.

---

## ğŸŒ API Documentation

### Live Endpoint

**Interactive Swagger Docs**: [https://souravmondal619-churn-mlops-api.hf.space/docs](https://souravmondal619-churn-mlops-api.hf.space/docs)

### Sample Request

**POST** `/predict`

```json
{
  "Customer ID": "0001-BGFD",
  "Monthly Charge": 75,
  "Total Revenue": 2800,
  "Tenure Months": 24,
  "Latitude": 40.7,
  "Longitude": -73.9,
  "Gender": "Male",
  "Senior Citizen": "No",
  "Internet Service": "Fiber Optic",
  "Contract": "Month-to-Month",
  "Payment Method": "Credit Card"
}
```

### Sample Response

```json
{
  "customer_id": "0001-BGFD",
  "prediction": "Joined",
  "churn_flag": "YES",
  "churn_probability": 0.48,
  "threshold_used": 0.25,
  "probabilities": {
    "Churned": 0.48,
    "Joined": 0.51,
    "Stayed": 0.01
  }
}
```

### Python Client Example

```python
import requests

url = "https://souravmondal619-churn-mlops-api.hf.space/predict"
data = {
    "Customer ID": "0001-BGFD",
    "Monthly Charge": 75,
    "Total Revenue": 2800,
    "Tenure Months": 24,
    "Latitude": 40.7,
    "Longitude": -73.9,
    "Gender": "Male",
    "Senior Citizen": "No",
    "Internet Service": "Fiber Optic",
    "Contract": "Month-to-Month",
    "Payment Method": "Credit Card"
}

response = requests.post(url, json=data)
print(response.json())
```

---

## ğŸ”„ Model Retraining, CI/CD & Deployment Flow (Crystal Clear)

This project is designed so that **model training, evaluation, batch prediction, and deployment are automated** as much as possible. Here is the complete picture ğŸ‘‡

### ğŸ“¥ 1ï¸âƒ£ How to Feed New Data Into the Model

Training data lives in:
```
data/churn_clean.xlsx
```

**To retrain with new data:**

```bash
# 1. Append or replace rows in the training dataset
cp your_new_data.xlsx data/churn_clean.xlsx

# 2. Commit and push your changes
git add data/churn_clean.xlsx
git commit -m "Update training data"
git push origin main
```

ğŸ”” **No manual retraining needed locally** â€” GitHub Actions handles it automatically!

### ğŸ¤– 2ï¸âƒ£ What Happens After You Push (CI/CD Pipeline)

When you push to `main`, the CI pipeline **automatically runs**:

**Pipeline Stages:**
```yaml
1ï¸âƒ£ Install dependencies
2ï¸âƒ£ Preprocessing (data cleaning + feature engineering)
3ï¸âƒ£ Model Training (with threshold optimization)
4ï¸âƒ£ Model Evaluation (metrics & validation)
5ï¸âƒ£ Upload artifacts to Hugging Face Model Hub
6ï¸âƒ£ Run batch predictions
7ï¸âƒ£ Build Docker API image
8ï¸âƒ£ Upload prediction results as GitHub artifacts
```

**Pipeline file location:**
```
.github/workflows/ci.yml
```

### ğŸš€ 3ï¸âƒ£ Does Hugging Face Model Update Automatically?

âœ… **YES â€” completely automatically!**

When training finishes, CI uploads artifacts directly to the HF Model Hub.

**Files updated:**
```
model.pkl
scaler.pkl
kmeans.pkl
label_encoder.pkl
feature_columns.pkl
churn_threshold.pkl
```

**Repository location:**
```
hf: souravmondal619/churn-mlops-model
```

**What this means:**
- When the model retrains â†’ Hugging Face model is updated automatically
- Since the API loads artifacts directly from Hugging Face â†’ **The deployed API always uses the latest trained model**

### ğŸ§© 4ï¸âƒ£ Does the API Deployment Update Too?

**Yes â€” indirectly and automatically!**

The API (`api.py`) loads models **dynamically** from Hugging Face:

```python
from huggingface_hub import hf_hub_download

model = pickle.load(open(hf_hub_download(REPO_ID, "model.pkl"), "rb"))
```

**So after CI uploads a new model:**
- â¡ï¸ Hugging Face API Space automatically starts using the new version
- â¡ï¸ **No manual re-deployment needed**

### ğŸ— 5ï¸âƒ£ What About the CI Pipeline Model?

The CI pipeline uses the same code, so it:
- âœ” Retrains the model
- âœ” Evaluates it
- âœ” Uploads it to Hugging Face
- âœ” Generates prediction artifacts

**Everything stays synchronized.**  
There is only **ONE source of truth** â€” the Hugging Face model repository.

### ğŸ” 6ï¸âƒ£ Connecting GitHub â†’ Hugging Face (via Token)

To let GitHub upload artifacts securely, we use a Hugging Face Access Token.

**Step 1 â€” Create Token in Hugging Face**

Go to: [Settings â†’ Access Tokens â†’ New Token](https://huggingface.co/settings/tokens)

**Permissions required:**
- âœ” Write access to repositories
- âœ” Read access (auto)
- Name it anything (e.g., `HF_TOKEN`)

Copy the generated token.

**Step 2 â€” Store Token in GitHub Secrets**

Navigate to: `GitHub â†’ Repository â†’ Settings â†’ Secrets â†’ Actions â†’ New repository secret`

```
Name: HF_TOKEN
Value: [Your Hugging Face token - keep this private!]
```

**Step 3 â€” CI Uses the Token Securely**

Inside `ci.yml`, we authenticate with Hugging Face:

```yaml
- name: Login to Hugging Face
  run: |
    python -c "from huggingface_hub import login; import os; login(token=os.environ['HF_TOKEN'])"
  env:
    HF_TOKEN: ${{ secrets.HF_TOKEN }}
```

**Now:**
- âœ” CI can upload models securely
- âœ” No one sees your token
- âœ” Fully automated and secure

### ğŸ“¬ 7ï¸âƒ£ Feeding JSON to API (For Inference)

**Swagger documentation:**
```
https://souravmondal619-churn-mlops-api.hf.space/docs
```

**Example request body:**
```json
{
  "Customer ID": "0001-BGFD",
  "Monthly Charge": 75,
  "Total Revenue": 2800,
  "Tenure Months": 24,
  "Latitude": 40.7,
  "Longitude": -73.9,
  "Gender": "Male",
  "Senior Citizen": "No",
  "Internet Service": "Fiber Optic",
  "Contract": "Month-to-Month",
  "Payment Method": "Credit Card"
}
```

**Example API response:**
```json
{
  "prediction": "Joined",
  "churn_flag": "YES",
  "churn_probability": 0.49,
  "threshold_used": 0.25
}
```

### ğŸ§¾ Summary (Bullet-Proof Clarity)

| Action | Result |
|--------|--------|
| Modify training data | Model retrains automatically |
| Push to GitHub | CI pipeline runs |
| Pipeline finishes | New model uploaded to HF |
| API calls | Always use latest model |
| GitHub Artifacts | Store batch prediction CSVs |
| HF Token | Secure bridge between GitHub â†’ HuggingFace |

**Everything is automated. No manual deployments. No duplicated models.**

---

## ğŸ“Š Business Intelligence

### Power BI Dashboard Insights

**Key findings from exploratory analysis:**

ğŸ“Œ **Contract type is the strongest churn indicator**
- Month-to-month contracts show highest churn rates
- Two-year contracts have 90% lower churn

âš ï¸ **High-risk customer segments**
- Tenure < 12 months: 45% churn rate
- Total charges > $5000 + complaints: 67% churn rate
- Fiber optic users with month-to-month contracts: 52% churn

ğŸ”Œ **Service-level patterns**
- Fiber optic internet: Higher churn (price sensitivity)
- Credit card auto-pay: 30% lower churn
- Senior citizens: 1.4x higher churn rate

ğŸ’¡ **Retention opportunities**
- Contract upgrades reduce churn by 65%
- Loyalty rewards effective for high-value customers
- Proactive support for first-year customers critical

### Dashboard Components

- Churn rate by contract type
- Tenure vs churn probability
- Revenue distribution across churn segments
- Service usage patterns
- Geographic churn clusters
- Payment method impact analysis

### EDA Findings

**From notebooks analysis:**

- **Revenue vs churn is non-linear**: High-value customers churn at different rates
- **Internet service type plays a major role**: Fiber optic shows unique patterns
- **Senior citizens churn at a higher rate**: 1.4x baseline rate
- **Missing values handling**: Systematic imputation strategies applied
- **Outlier detection**: Charges, downloads, revenue carefully examined
- **Correlation heatmaps**: Strong correlations between contract/tenure/churn

---

## ğŸ¤– Model Details

### Data Preprocessing

**Feature Engineering:**
- Geospatial clustering (latitude/longitude â†’ cluster IDs)
- Categorical encoding (one-hot + label encoding)
- Numerical scaling (StandardScaler)
- Missing value imputation
- Outlier detection and handling

**Transformations:**
- Revenue binning
- Tenure categorization
- Interaction features (contract Ã— service type)

### Model Training

**Algorithms Evaluated:**
- Logistic Regression (baseline)
- Random Forest
- Gradient Boosting
- XGBoost

**Selection Criteria:**
- Cross-validated accuracy
- Recall on churn class (minimize false negatives)
- Model interpretability
- Inference latency

**Final Model**: Gradient Boosting (best balance of performance and explainability)

### Threshold Optimization

Instead of using the default 0.5 probability threshold, we optimize using:

1. **ROC Curve Analysis**: Find operating point balancing TPR/FPR
2. **Youden's J Statistic**: Maximize (Sensitivity + Specificity - 1)
3. **Business Cost Sensitivity**: Account for retention cost vs churn cost

**Result**: Optimized threshold (~0.25) saved as `churn_threshold.pkl`

**Benefits:**
- Consistent predictions across all environments
- Better alignment with business objectives
- Reduced false negatives (missed churners)

### Model Performance

**Validation Metrics** (typical results):
- Accuracy: 82%
- Precision (Churn): 74%
- Recall (Churn): 81%
- F1-Score: 77%
- ROC-AUC: 0.87

---

## ğŸš€ Deployment

### Hugging Face Spaces

**Why Hugging Face?**
- Serverless deployment (zero infrastructure management)
- Automatic scaling
- Built-in monitoring
- Free tier available
- Direct integration with Model Hub

**Deployment Process:**

1. **CI uploads artifacts** â†’ Hugging Face Model Hub
2. **FastAPI loads artifacts** dynamically at runtime
3. **Spaces hosts API** with automatic SSL/DNS
4. **Users access** via public endpoint


### Docker Deployment

**Why Docker?**

This project uses a **Dockerfile** so the API runs the **same way everywhere** â€” locally, on Hugging Face Spaces, or on any cloud â€” **without breaking**.

**Benefits:**
- âœ… **Environment consistency**: Same dependencies, same Python version, same behavior
- âœ… **Reproducibility**: No "works on my machine" issues
- âœ… **Portability**: Deploy anywhere that supports Docker (AWS, GCP, Azure, HF Spaces)
- âœ… **Isolation**: Clean, containerized environment

**Build and run locally:**

```bash
# Build image
docker build -t churn-api .

# Run container
docker run -p 8000:8000 churn-api

# Access API
curl http://localhost:8000/docs
```

**Hugging Face Spaces uses the same Dockerfile**, ensuring identical behavior between your local development and production deployment.

### Automatic Synchronization Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Developer pushes code/data                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. GitHub Actions triggers CI pipeline                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Model retrains with new data                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Artifacts upload to Hugging Face Model Hub                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Production API automatically uses new model               â”‚
â”‚    (no restart required)                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Summary Table:**

| Action | Result |
|--------|--------|
| Modify training data | Model retrains automatically |
| Push to GitHub | CI pipeline runs |
| Pipeline finishes | New model uploaded to HF |
| API calls | Always use latest model |
| GitHub Artifacts | Store batch prediction CSVs |
| HF Token | Secure bridge between GitHub â†’ HuggingFace |

**No manual deployment needed. Ever.**

---

## ğŸ¤ CrewAI Recommendation Agent Implementation In Notebook 

### Purpose

Transform predictions into actionable retention strategies using AI-powered analysis.

### How It Works

1. **Input**: Customer profile + churn probability
2. **Analysis**: CrewAI agent evaluates risk factors
3. **Output**: Personalized retention recommendations

### Sample Recommendations

**For high-risk month-to-month customer:**
- Offer 20% discount on annual contract upgrade
- Enroll in loyalty rewards program
- Schedule proactive support call
- Provide fiber optic speed boost trial

**For moderate-risk customer:**
- Highlight contract upgrade benefits
- Offer flexible payment options
- Send personalized retention email

### Integration Example
```
strategy_agent = Agent(
    role="Telecom Retention Strategist",
    goal="Provide retention recommendations based on churn risk",
    backstory="You are a senior telecom business strategist...",
    llm=llm,
    verbose=False
)

for _, r in results.iterrows():
    if risk in ["Medium Risk", "High Risk"]:
        task = Task(
            description=f"Customer churn probability: {churn_prob:.2f}, Risk: {risk}",
            expected_output="One concise retention recommendation.",
            agent=strategy_agent
        )
        recommendation = Crew(agents=[strategy_agent], tasks=[task]).kickoff()
```

---

## ğŸ“§ Contact

**Project Maintainer**: Sourav Mondal  

---